model:
  model_name: "meta-llama/Llama-3.2-1B"
  use_4bit: true
  torch_dtype: "bfloat16"
  lora_r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","down_proj","up_proj"]
